services:
  master:
    image: hadoop_base:1.0
    container_name: master
    hostname: master
    user: root
    environment:
      - HADOOP_HOME=/opt/hadoop
      # - HDFS_NAMENODE_USER=root
      # - HDFS_DATANODE_USER=root
      # - HDFS_SECONDARYNAMENODE_USER=root
      # - YARN_RESOURCEMANAGER_USER=root
      # - YARN_NODEMANAGER_USER=root
      # - HADOOP_CONF_DIR=/opt/hadoop/etc/hadoop
    volumes:
      - namenode:/opt/hadoop/data/nameNode
      - config_namenode:/opt/hadoop/etc/hadoop
      - config_master:/opt/spark/conf
      # - workspace:/workspace
      - ./hadoop/entrypoint.sh:/entrypoint.sh
    ports:
      # - "8032:8032"
      - "9870:9870" 
      - "8088:8088"
      - "7077:7077"
      - "9000:9000"
    # command: [ "/bin/bash", "/start-hdfs.sh" ]
    entrypoint: ["/entrypoint.sh", "master"]
    networks:
      hadoop_network:
        ipv4_address: 172.20.0.2

  worker1:
    image: hadoop_base:1.0
    container_name: worker1
    hostname: worker1
    user: root
    environment:
      - HADOOP_HOME=/opt/hadoop
      # - HDFS_NAMENODE_USER=root
      # - HDFS_DATANODE_USER=root
      # - HDFS_SECONDARYNAMENODE_USER=root
      # - YARN_RESOURCEMANAGER_USER=root
      # - YARN_NODEMANAGER_USER=root
      # - HADOOP_CONF_DIR=/opt/hadoop/etc/hadoop
    volumes:
      - datanode1:/opt/hadoop/data/dataNode
      - config_datanode:/opt/hadoop/etc/hadoop
      - config_worker:/opt/spark/conf
      - ./hadoop/entrypoint.sh:/entrypoint.sh
    depends_on:
      - master
    # command: [ "/bin/bash", "-c", "/init-datanode.sh && /opt/hadoop/sbin/start_all.sh" ]
    entrypoint: ["/entrypoint.sh", "worker"]
    networks:
      hadoop_network:
        ipv4_address: 172.20.0.3

  worker2:
    image: hadoop_base:1.0
    container_name: worker2
    hostname: worker2
    user: root
    environment:
      - HADOOP_HOME=/opt/hadoop
      # - HDFS_NAMENODE_USER=root
      # - HDFS_DATANODE_USER=root
      # - HDFS_SECONDARYNAMENODE_USER=root
      # - YARN_RESOURCEMANAGER_USER=root
      # - YARN_NODEMANAGER_USER=root
      # - HADOOP_CONF_DIR=/opt/hadoop/etc/hadoop
    volumes:
      - datanode2:/opt/hadoop/data/dataNode
      - config_datanode:/opt/hadoop/etc/hadoop
      - config_master:/opt/spark/conf     
      - ./hadoop/entrypoint.sh:/entrypoint.sh
    depends_on:
      - master
    entrypoint: ["/entrypoint.sh", "worker"]
    networks:
      hadoop_network:
        ipv4_address: 172.20.0.4
  
  history_server:
    image: hadoop_base:1.0
    container_name: history_server
    hostname: history_server
    user: root
    ports:
      - "18080:18080"

    volumes:
      - config_namenode:/opt/hadoop/etc/hadoop
      - config_master:/opt/spark/conf 
      - ./hadoop/entrypoint.sh:/entrypoint.sh
    depends_on:
      - master
    entrypoint: ["/entrypoint.sh", "history"]
    networks:
      hadoop_network:
        ipv4_address: 172.20.0.5

  livy:
    image: hadoop_base:1.0
    container_name: livy
    hostname: livy
    user: root
    ports:
      - "8998:8998"
    volumes:
      - config_namenode:/opt/hadoop/etc/hadoop
      - config_master:/opt/spark/conf 
      - config_livy:/opt/livy/conf
      - ./hadoop/entrypoint.sh:/entrypoint.sh
    depends_on:
      - master
      - worker1
      - worker2
    command: ["/entrypoint.sh", "livy"]
    networks:
      hadoop_network:
        ipv4_address: 172.20.0.6
  
  standby_master:
    image: hadoop_base:1.0
    container_name: standby_master
    hostname: standby_master
    user: root
    environment:
      - HADOOP_HOME=/opt/hadoop
      # - HDFS_NAMENODE_USER=root
      # - HDFS_DATANODE_USER=root
      # - HDFS_SECONDARYNAMENODE_USER=root
      # - YARN_RESOURCEMANAGER_USER=root
      # - YARN_NODEMANAGER_USER=root
      # - HADOOP_CONF_DIR=/opt/hadoop/etc/hadoop
    volumes:
      - standby_namenode:/opt/hadoop/data/nameNode
      - config_namenode:/opt/hadoop/etc/hadoop
      - config_master:/opt/spark/conf
      # - workspace:/workspace
      - ./hadoop/entrypoint.sh:/entrypoint.sh
    # ports:
    #   - "9870:9870" 
    #   - "8088:8088"
    #   - "7077:7077"
    #   - "9000:9000"
    # command: [ "/bin/bash", "/start-hdfs.sh" ]
    depends_on:
      - master
    entrypoint: ["/entrypoint.sh", "standby"]
    networks:
      hadoop_network:
        ipv4_address: 172.20.0.7

volumes:
  namenode:
    driver: local
    driver_opts:
      o: bind
      type: none
      device: ./hadoop/namenode
  standby_namenode:
    driver: local
    driver_opts:
      o: bind
      type: none
      device: ./hadoop/standby_namenode
  datanode1:
    driver: local
    driver_opts:
      o: bind
      type: none
      device: ./hadoop/datanode1
  datanode2:
    driver: local
    driver_opts:
      o: bind
      type: none
      device: ./hadoop/datanode2
  config_namenode:
    driver: local
    driver_opts:
      o: bind
      type: none
      device: ./hadoop/config_namenode
  config_datanode:
    driver: local
    driver_opts:
      o: bind
      type: none
      device: ./hadoop/config_datanode
  config_master:
    driver: local
    driver_opts:
      o: bind
      type: none
      device: ./spark/config_master
  config_worker:
    driver: local
    driver_opts:
      o: bind
      type: none
      device: ./spark/config_worker
  config_livy:
    driver: local
    driver_opts:
      o: bind
      type: none
      device: ./livy/config
  workspace:
    driver: local
    driver_opts:
      o: bind
      type: none
      device: ./spark/workspace

networks:
  hadoop_network:
    ipam:
      driver: default
      config:
        - subnet: 172.20.0.0/16
        
