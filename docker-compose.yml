services:
  namenode:
    image: hadoop_base:1.0
    container_name: namenode
    hostname: namenode
    user: root
    volumes:
      - namenode:/var/data/hadoop/hdfs
      - config_hadoop:/opt/hadoop/etc/hadoop
      - ./hadoop/entrypoint.sh:/entrypoint.sh
    ports:
      # - "8020:8020"
      # - "9000:9000"
      - "9870:9870"
    entrypoint: ["/entrypoint.sh", "namenode"]
    networks:
      hadoop_network:
        ipv4_address: ${namenode_ip}
  
  resourcemanager:
    image: hadoop_base:1.0
    container_name: resourcemanager
    hostname: resourcemanager
    user: root
    volumes:
      - resourcemanger:/var/data/hadoop/logs
      - config_hadoop:/opt/hadoop/etc/hadoop
      - ./hadoop/entrypoint.sh:/entrypoint.sh
    ports:
      - "8088:8088" #Resource Manager WebUI
    entrypoint: ["/entrypoint.sh", "resourcemanager"]
    networks:
      hadoop_network:
        ipv4_address: ${resourcemanager_ip}
  secondarynamenode:
    image: hadoop_base:1.0
    container_name: secondarynamenode
    hostname: secondarynamenode
    user: root
    volumes:
      - secondarynamenode:/var/data/hadoop/hdfs
      - config_hadoop:/opt/hadoop/etc/hadoop
      - ./hadoop/entrypoint.sh:/entrypoint.sh
    # ports:
    #   - "50090:50090" #secondary namenode metadata service
    entrypoint: ["/entrypoint.sh", "secondarynamenode"]
    networks:
      hadoop_network:
        ipv4_address: ${secondarynamenode_ip}

  worker1:
    image: hadoop_base:1.0
    container_name: worker1
    hostname: worker1
    user: root
    # environment:
    #   - HADOOP_HOME=/opt/hadoop
    volumes:
      - worker1:/var/data/hadoop/hdfs
      - config_hadoop:/opt/hadoop/etc/hadoop
      # - config_worker:/opt/spark/conf
      - ./hadoop/entrypoint.sh:/entrypoint.sh
    depends_on:
      - namenode
      - resourcemanager
      # - secondarynamenode
    entrypoint: ["/entrypoint.sh", "worker"]
    networks:
      hadoop_network:
        ipv4_address: ${worker1_ip}

  worker2:
    image: hadoop_base:1.0
    container_name: worker2
    hostname: worker2
    user: root
    # environment:
    #   - HADOOP_HOME=/opt/hadoop
    volumes:
      - worker2:/var/data/hadoop/hdfs
      - config_hadoop:/opt/hadoop/etc/hadoop
      # - config_worker:/opt/spark/conf
      - ./hadoop/entrypoint.sh:/entrypoint.sh
    depends_on:
      - namenode
      - resourcemanager
      # - secondarynamenode
    entrypoint: ["/entrypoint.sh", "worker"]
    networks:
      hadoop_network:
        ipv4_address: ${worker2_ip}

  worker3:
    image: hadoop_base:1.0
    container_name: worker3
    hostname: worker3
    user: root
    # environment:
    #   - HADOOP_HOME=/opt/hadoop
    volumes:
      - worker3:/var/data/hadoop/hdfs
      - config_hadoop:/opt/hadoop/etc/hadoop
      # - config_worker:/opt/spark/conf
      - ./hadoop/entrypoint.sh:/entrypoint.sh
    depends_on:
      - namenode
      - resourcemanager
      # - secondarynamenode
    entrypoint: ["/entrypoint.sh", "worker"]
    networks:
      hadoop_network:
        ipv4_address: ${worker3_ip}
  
  historyserver:
    image: hadoop_base:1.0
    container_name: historyserver
    hostname: historyserver
    user: root
    # ports:
    #   - "18080:18080" 
    volumes:
      - config_hadoop:/opt/hadoop/etc/hadoop
      - ./hadoop/entrypoint.sh:/entrypoint.sh
    depends_on:
      - namenode
    entrypoint: ["/entrypoint.sh", "historyserver"]
    networks:
      hadoop_network:
        ipv4_address: ${historyserver_ip}

  # edgenode:
  #   image: hadoop_base:1.0
  #   container_name: edgenode
  #   hostname: edgenode
  #   user: root
  #   volumes:
  #     - config_hadoop:/opt/hadoop/etc/hadoop
  #     - config_master:/opt/spark/conf 
  #     - workspace:/workspace    
  #     - ./hadoop/entrypoint.sh:/entrypoint.sh
  #   depends_on:
  #     - master
  #     - worker1
  #     - worker2
  #   command: ["/entrypoint.sh", ""]
  #   networks:
  #     hadoop_network:
  #       ipv4_address: ${edgenode_ip}

volumes:
  namenode:
    driver: local
    driver_opts:
      o: bind
      type: none
      device: ./hadoop/nn
  secondarynamenode:
    driver: local
    driver_opts:
      o: bind
      type: none
      device: ./hadoop/snn
  resourcemanger:
    driver: local
    driver_opts:
      o: bind
      type: none
      device: ./yarn/logs
  worker1:
    driver: local
    driver_opts:
      o: bind
      type: none
      device: ./hadoop/worker1
  worker2:
    driver: local
    driver_opts:
      o: bind
      type: none
      device: ./hadoop/worker2
  worker3:
    driver: local
    driver_opts:
      o: bind
      type: none
      device: ./hadoop/worker3
  config_hadoop:
    driver: local
    driver_opts:
      o: bind
      type: none
      device: ./hadoop/config_hadoop

networks:
  hadoop_network:
    ipam:
      driver: default
      config:
        - subnet: 172.20.0.0/16
        
