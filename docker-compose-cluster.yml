services:
  namenode:
    image: hadoop_base:1.0
    container_name: namenode
    hostname: namenode
    user: root
    volumes:
      - namenode:/var/data/hadoop/hdfs
      - config_hadoop:/opt/hadoop/etc/hadoop
      - config_spark:/opt/spark/conf
      - ./hadoop/entrypoint.sh:/entrypoint.sh
    ports:
      # - "8020:8020"
      - "9000:9000"
      - "9870:9870" # hdfs WebUI
    entrypoint: ["/entrypoint.sh", "namenode"]
    networks:
      hadoop_network:
        ipv4_address: ${namenode_ip}
  
  resourcemanager:
    image: hadoop_base:1.0
    container_name: resourcemanager
    hostname: resourcemanager
    user: root
    volumes:
      - resourcemanager:/var/data/hadoop/hdfs
      - config_hadoop:/opt/hadoop/etc/hadoop
      - config_spark:/opt/spark/conf 
      - ./hadoop/entrypoint.sh:/entrypoint.sh
    ports:
      - "8088:8088" #Resource Manager WebUI
      - "8032:8032" #Resource Manager WebUI
    entrypoint: ["/entrypoint.sh", "resourcemanager"]
    networks:
      hadoop_network:
        ipv4_address: ${resourcemanager_ip}
  secondarynamenode:
    image: hadoop_base:1.0
    container_name: secondarynamenode
    hostname: secondarynamenode
    user: root
    volumes:
      - secondarynamenode:/var/data/hadoop/hdfs
      - config_hadoop:/opt/hadoop/etc/hadoop
      - config_spark:/opt/spark/conf 
      - ./hadoop/entrypoint.sh:/entrypoint.sh
    # ports:
    #   - "50090:50090" #secondary namenode metadata service
    entrypoint: ["/entrypoint.sh", "secondarynamenode"]
    networks:
      hadoop_network:
        ipv4_address: ${secondarynamenode_ip}

  worker1:
    image: hadoop_base:1.0
    container_name: worker1
    hostname: worker1
    user: root
    # environment:
    #   - HADOOP_HOME=/opt/hadoop
    volumes:
      - worker1:/var/data/hadoop/hdfs
      - config_hadoop:/opt/hadoop/etc/hadoop
      - config_spark:/opt/spark/conf
      - ./hadoop/entrypoint.sh:/entrypoint.sh
    depends_on:
      - namenode
      - resourcemanager
      # - secondarynamenode
    entrypoint: ["/entrypoint.sh", "worker"]
    networks:
      hadoop_network:
        ipv4_address: ${worker1_ip}

  worker2:
    image: hadoop_base:1.0
    container_name: worker2
    hostname: worker2
    user: root
    volumes:
      - worker2:/var/data/hadoop/hdfs
      - config_hadoop:/opt/hadoop/etc/hadoop
      - config_spark:/opt/spark/conf 
      - ./hadoop/entrypoint.sh:/entrypoint.sh
    depends_on:
      - namenode
      - resourcemanager
      # - secondarynamenode
    entrypoint: ["/entrypoint.sh", "worker"]
    networks:
      hadoop_network:
        ipv4_address: ${worker2_ip}

  worker3:
    image: hadoop_base:1.0
    container_name: worker3
    hostname: worker3
    user: root
    # environment:
    #   - HADOOP_HOME=/opt/hadoop
    volumes:
      - worker3:/var/data/hadoop/hdfs
      - config_hadoop:/opt/hadoop/etc/hadoop
      - config_spark:/opt/spark/conf 
      - ./hadoop/entrypoint.sh:/entrypoint.sh
    depends_on:
      - namenode
      - resourcemanager
      # - secondarynamenode
    entrypoint: ["/entrypoint.sh", "worker"]
    networks:
      hadoop_network:
        ipv4_address: ${worker3_ip}
  
  historyserver:
    image: hadoop_base:1.0
    container_name: historyserver
    hostname: historyserver
    user: root
    ports:
      - "18080:18080" #Spark History Server UI
      - "19888:19888" #Mapreduce History Server UI
    volumes:
      - config_hadoop:/opt/hadoop/etc/hadoop
      - config_spark:/opt/spark/conf
      - historyserver:/opt/spark/logs
      - ./hadoop/entrypoint.sh:/entrypoint.sh
    depends_on:
      - namenode
      - secondarynamenode
      - resourcemanager
      - worker1
      - worker2
      - worker3
      # - edgenode
    entrypoint: ["/entrypoint.sh", "historyserver"]
    networks:
      hadoop_network:
        ipv4_address: ${historyserver_ip}

  # edgenode:
  #   image: hadoop_base:1.0
  #   container_name: edgenode
  #   hostname: edgenode
  #   user: root
  #   volumes:
  #     - config_hadoop:/opt/hadoop/etc/hadoop
  #     - config_spark:/opt/spark/conf 
  #     - workspace:/workspace    
  #     - ./hadoop/entrypoint.sh:/entrypoint.sh
  #   depends_on:
  #     - namenode
  #     - secondarynamenode
  #     - resourcemanager
  #     - worker1
  #     - worker2
  #     - worker3
  #   command: ["/entrypoint.sh", "edgenode"]
  #   networks:
  #     hadoop_network:
  #       ipv4_address: ${edgenode_ip}

volumes:
  namenode:
    driver: local
    driver_opts:
      o: bind
      type: none
      device: ./hadoop/namenode
  secondarynamenode:
    driver: local
    driver_opts:
      o: bind
      type: none
      device: ./hadoop/secondarynamenode
  resourcemanager:
    driver: local
    driver_opts:
      o: bind
      type: none
      device: ./hadoop/resourcemanager
  historyserver:
    driver: local
    driver_opts:
      o: bind
      type: none
      device: ./hadoop/historyserver
  worker1:
    driver: local
    driver_opts:
      o: bind
      type: none
      device: ./hadoop/worker1
  worker2:
    driver: local
    driver_opts:
      o: bind
      type: none
      device: ./hadoop/worker2
  worker3:
    driver: local
    driver_opts:
      o: bind
      type: none
      device: ./hadoop/worker3
  workspace:
    driver: local
    driver_opts:
      o: bind
      type: none
      device: ./edge/workspace
  config_hadoop:
    driver: local
    driver_opts:
      o: bind
      type: none
      device: ./hadoop/config_hadoop
  config_spark:
    driver: local
    driver_opts:
      o: bind
      type: none
      device: ./edge/config_spark

networks:
  hadoop_network:
    ipam:
      driver: default
      config:
        - subnet: 172.20.0.0/16
        
